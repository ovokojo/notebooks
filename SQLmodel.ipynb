{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to SQL Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) are designed to solve sequence modelling problems like translations, because they process information incrementally and can maintain an internal state that uses past information to inform predictions.\n",
    "\n",
    "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network that has LSTM cell blocks instead of typical neural network layers. The main components of these cells are the input gate, the forget gate and the output gate. This architecture solves the <a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\">vanishing gradient problem</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='static/LSTM.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/\">Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to build and train a bi-directional Long Short-Term Memory (LSTM) model for mapping text to SQL. This baseline approach works by identifying an SQL template for the question, then predicting appropriate words to fill slots in the template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains natural language questions and corresponding SQL queries about course information at the University of Michigan.\n",
    "Some questions were collected from the department Facebook page and others were written by students familiar with the database, instructed to write questions they might ask an academic advisor. The authors of the dataset manually labeled the questions with SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dynet as dy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-split</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sql</th>\n",
       "      <th>variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>[{'question-split': 'train', 'text': 'Can unde...</td>\n",
       "      <td>[SELECT DISTINCT COURSEalias0.ADVISORY_REQUIRE...</td>\n",
       "      <td>[{'example': 'EECS', 'location': 'sql-only', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>[{'question-split': 'test', 'text': 'What 's t...</td>\n",
       "      <td>[SELECT DISTINCT COURSEalias0.DEPARTMENT , COU...</td>\n",
       "      <td>[{'example': 'MDE', 'location': 'both', 'name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>[{'question-split': 'test', 'text': 'Can I tak...</td>\n",
       "      <td>[SELECT COUNT( * ) &gt; 0 FROM COURSE AS COURSEal...</td>\n",
       "      <td>[{'example': '370', 'location': 'both', 'name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>[{'question-split': 'test', 'text': 'Can you s...</td>\n",
       "      <td>[SELECT DISTINCT COURSEalias0.DEPARTMENT , COU...</td>\n",
       "      <td>[{'example': 'EECS', 'location': 'both', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>[{'question-split': 'train', 'text': 'Who usua...</td>\n",
       "      <td>[SELECT COUNT( INSTRUCTORalias0.NAME ) , INSTR...</td>\n",
       "      <td>[{'example': 'EECS', 'location': 'both', 'name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query-split                                          sentences  \\\n",
       "0        test  [{'question-split': 'train', 'text': 'Can unde...   \n",
       "1       train  [{'question-split': 'test', 'text': 'What 's t...   \n",
       "2        test  [{'question-split': 'test', 'text': 'Can I tak...   \n",
       "3        test  [{'question-split': 'test', 'text': 'Can you s...   \n",
       "4        test  [{'question-split': 'train', 'text': 'Who usua...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  [SELECT DISTINCT COURSEalias0.ADVISORY_REQUIRE...   \n",
       "1  [SELECT DISTINCT COURSEalias0.DEPARTMENT , COU...   \n",
       "2  [SELECT COUNT( * ) > 0 FROM COURSE AS COURSEal...   \n",
       "3  [SELECT DISTINCT COURSEalias0.DEPARTMENT , COU...   \n",
       "4  [SELECT COUNT( INSTRUCTORalias0.NAME ) , INSTR...   \n",
       "\n",
       "                                           variables  \n",
       "0  [{'example': 'EECS', 'location': 'sql-only', '...  \n",
       "1  [{'example': 'MDE', 'location': 'both', 'name'...  \n",
       "2  [{'example': '370', 'location': 'both', 'name'...  \n",
       "3  [{'example': 'EECS', 'location': 'both', 'name...  \n",
       "4  [{'example': 'EECS', 'location': 'both', 'name...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advising = pd.read_json('txt2sql-data/advising.json')\n",
    "advising.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question-split': 'test',\n",
       " 'text': 'Am I able to take department0 number0 and department0 number1 in the same semester ?',\n",
       " 'variables': {'department0': 'RCIDIV', 'number0': '316', 'number1': '224'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample Question:')\n",
    "advising['sentences'][2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample SQL:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SELECT COUNT( * ) > 0 FROM COURSE AS COURSEalias0 , COURSE AS COURSEalias1 , COURSE_OFFERING AS COURSE_OFFERINGalias0 , COURSE_OFFERING AS COURSE_OFFERINGalias1 WHERE COURSE_OFFERINGalias1.SEMESTER = COURSE_OFFERINGalias0.SEMESTER AND COURSEalias0.COURSE_ID = COURSE_OFFERINGalias0.COURSE_ID AND COURSEalias0.DEPARTMENT = \"department0\" AND COURSEalias0.NUMBER = number0 AND COURSEalias1.COURSE_ID = COURSE_OFFERINGalias1.COURSE_ID AND COURSEalias1.DEPARTMENT = \"department0\" AND COURSEalias1.NUMBER = number1 AND ( NOT ( ( COURSEalias1.COURSE_ID = COURSE_PREREQUISITEalias0.COURSE_ID AND COURSEalias0.COURSE_ID = COURSE_PREREQUISITEalias0.PRE_COURSE_ID ) OR ( COURSEaliasr02.COURSE_ID = COURSE_PREREQUISITEalias0.PRE_COURSE_ID AND COURSEalias1.COURSE_ID = COURSE_PREREQUISITEalias0.COURSE_ID ) ) ) ;']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample SQL:')\n",
    "advising['sql'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the process is to identify the variable names, SQL template, and complete sql statements for all the data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_variables(sql, sql_variables, sent, sent_variables):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    seen_sent_variables = set()\n",
    "    for token in sent.strip().split():\n",
    "        if (token not in sent_variables):\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            assert len(sent_variables[token]) > 0\n",
    "            seen_sent_variables.add(token)\n",
    "            for word in sent_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "\n",
    "    sql_tokens = []\n",
    "    for token in sql.strip().split():\n",
    "        if token.startswith('\"%') or token.startswith(\"'%\"):\n",
    "            sql_tokens.append(token[:2])\n",
    "            token = token[2:]\n",
    "        elif token.startswith('\"') or token.startswith(\"'\"):\n",
    "            sql_tokens.append(token[0])\n",
    "            token = token[1:]\n",
    "\n",
    "        if token.endswith('%\"') or token.endswith(\"%'\"):\n",
    "            sql_tokens.append(token[:-2])\n",
    "            sql_tokens.append(token[-2:])\n",
    "        elif token.endswith('\"') or token.endswith(\"'\"):\n",
    "            sql_tokens.append(token[:-1])\n",
    "            sql_tokens.append(token[-1])\n",
    "        else:\n",
    "            sql_tokens.append(token)\n",
    "\n",
    "    template = []\n",
    "    complete = []\n",
    "    for token in sql_tokens:\n",
    "        # Do the template\n",
    "        if token in seen_sent_variables:\n",
    "            # The token is a variable name that will be copied from the sentence\n",
    "            template.append(token)\n",
    "        elif (token not in sent_variables) and (token not in sql_variables):\n",
    "            # The token is an SQL keyword\n",
    "            template.append(token)\n",
    "        elif token in sent_variables and sent_variables[token] != '':\n",
    "            # The token is a variable whose value is unique to this questions,\n",
    "            # but is not explicitly given\n",
    "            template.append(sent_variables[token])\n",
    "        else:\n",
    "            # The token is a variable whose value is not unique to this\n",
    "            # question and not explicitly given\n",
    "            template.append(sql_variables[token])\n",
    "\n",
    "        # Do the complete case\n",
    "        if token in sent_variables and sent_variables[token] != '':\n",
    "            complete.append(sent_variables[token])\n",
    "        elif token in sql_variables:\n",
    "            complete.append(sql_variables[token])\n",
    "        else:\n",
    "            complete.append(token)\n",
    "\n",
    "    return (tokens, tags, ' '.join(template), ' '.join(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_data_for_query(data):\n",
    "    # By default, set to the query split value\n",
    "    dataset = data['query-split']\n",
    "    # split can be adjusted for small datasets that require cross-validation\n",
    "    split = None\n",
    "    use_all_sql = True\n",
    "    \n",
    "    if split is not None:\n",
    "        if str(split) == str(dataset):\n",
    "            dataset = \"test\"\n",
    "        else:\n",
    "            dataset = \"train\"\n",
    "\n",
    "    for sent_info in data['sentences']:\n",
    "        # For question split, set to this question's value\n",
    "        dataset = sent_info['question-split']\n",
    "        if split is not None:\n",
    "            if str(split) == str(dataset):\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                dataset = \"train\"\n",
    "\n",
    "        for sql in data['sql']:\n",
    "            sql_vars = {}\n",
    "            for sql_var in data['variables']:\n",
    "                sql_vars[sql_var['name']] = sql_var['example']\n",
    "            text = sent_info['text']\n",
    "            text_vars = sent_info['variables']\n",
    "\n",
    "            yield (dataset, insert_variables(sql, sql_vars, text, text_vars))\n",
    "\n",
    "            if not use_all_sql:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that make up the sentences and SQL statements need to be mapped to integers in order to be fed to the model. This vocabulary is built using a dictionary with keys as integers and words as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = {}\n",
    "        for word in corpus:\n",
    "            w2i.setdefault(word, len(w2i))\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.w2i.keys())\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counts = Counter()\n",
    "    words = {\"<UNK>\"}\n",
    "    tag_set = set()\n",
    "    template_set = set()\n",
    "    for tokens, tags, template, complete in train:\n",
    "        template_set.add(template)\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    for word in counts:\n",
    "        if counts[word] > 0:\n",
    "            words.add(word)\n",
    "\n",
    "    vocab_tags = Vocab.from_corpus(tag_set)\n",
    "    vocab_words = Vocab.from_corpus(words)\n",
    "    vocab_templates = Vocab.from_corpus(template_set)\n",
    "\n",
    "    return vocab_words, vocab_tags, vocab_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the dataset is split into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "with open('txt2sql-data/advising.json') as input_file:\n",
    "    data = json.load(input_file)\n",
    "    for example in data:\n",
    "        for dataset, instance in get_tagged_data_for_query(example):\n",
    "            if dataset == 'train':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'dev':\n",
    "                train.append(instance)\n",
    "            elif dataset == 'test':\n",
    "                test.append(instance)\n",
    "            elif dataset == 'exclude':\n",
    "                pass\n",
    "            else:\n",
    "                assert False, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of data that comes out at the end of the preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', 'undergrads', 'take', '550', '?']\n",
      "['O', 'O', 'O', 'number0', 'O']\n",
      "SELECT DISTINCT COURSEalias0.ADVISORY_REQUIREMENT , COURSEalias0.ENFORCED_REQUIREMENT , COURSEalias0.NAME FROM COURSE AS COURSEalias0 WHERE COURSEalias0.DEPARTMENT = \" EECS \" AND COURSEalias0.NUMBER = number0 ;\n",
      "SELECT DISTINCT COURSEalias0.ADVISORY_REQUIREMENT , COURSEalias0.ENFORCED_REQUIREMENT , COURSEalias0.NAME FROM COURSE AS COURSEalias0 WHERE COURSEalias0.DEPARTMENT = \" EECS \" AND COURSEalias0.NUMBER = 550 ;\n"
     ]
    }
   ],
   "source": [
    "for tokens, tags, template, complete in train:\n",
    "    print(tokens)\n",
    "    print(tags)\n",
    "    print(template)\n",
    "    print(complete)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary:\n",
      "219 templates 2591 words 23 tags 2537 unknown words\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags, vocab_templates = build_vocab(train)\n",
    "UNK = vocab_words.w2i[\"<UNK>\"]\n",
    "NWORDS = vocab_words.size()\n",
    "NTAGS = vocab_tags.size()\n",
    "NTEMPLATES = vocab_templates.size()\n",
    "\n",
    "print(f\"Data Summary:\")\n",
    "print(f\"{NTEMPLATES} templates\", f\"{NWORDS} words\", f\"{NTAGS} tags\", f\"{UNK} unknown words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was built using DyNet, a neural network library developed by Carnegie Mellon University and many others.\n",
    "It is designed to run efficiently on CPUs or GPUs, and is very useful for natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "\n",
    "# This trainer performs stochastic gradient descent\n",
    "# this is the most common optimization procedure for neural networks.\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "# Input dimension \n",
    "DIM_WORD = 64\n",
    "\n",
    "# Number of input layers\n",
    "LSTM_LAYERS = 2\n",
    "\n",
    "# Dimension of the recurrent units\n",
    "DIM_HIDDEN_LSTM = 128\n",
    "\n",
    "# Dimension of the hidden layers\n",
    "DIM_HIDDEN_MLP = 32\n",
    "\n",
    "# Dimension of the hidden layers that predict the template\n",
    "DIM_HIDDEN_TEMPLATE = 32\n",
    "\n",
    "# Word Embeddings\n",
    "pEmbedding = model.add_lookup_parameters((NWORDS, DIM_WORD))\n",
    "\n",
    "# Layers  \n",
    "pHidden = model.add_parameters((DIM_HIDDEN_MLP, DIM_HIDDEN_LSTM*2))\n",
    "pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_MLP))\n",
    "\n",
    "# This allows us to create a standard LSTM\n",
    "builders = [\n",
    "    dy.LSTMBuilder(LSTM_LAYERS, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "    dy.LSTMBuilder(LSTM_LAYERS, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "]\n",
    "\n",
    "pHiddenTemplate = model.add_parameters((DIM_HIDDEN_TEMPLATE, DIM_HIDDEN_LSTM*2))\n",
    "pOutputTemplate = model.add_parameters((NTEMPLATES, DIM_HIDDEN_TEMPLATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(words, tags, template, builders, train=True):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    dropout_rate = 0\n",
    "    mlp = True\n",
    "\n",
    "    if train and dropout_rate is not None and dropout_rate > 0:\n",
    "        for b in builders:\n",
    "            b.set_dropouts(dropout_rate, dropout_rate)\n",
    "\n",
    "    f_init, b_init = [b.initial_state() for b in builders]\n",
    "\n",
    "    wembs = [dy.lookup(pEmbedding, w) for w in words]\n",
    "    if train: # Add noise in training as a regularizer\n",
    "        wembs = [dy.noise(we, 0.1) for we in wembs]\n",
    "\n",
    "    fw_states = [x for x in f_init.add_inputs(wembs)]\n",
    "    bw_states = [x for x in b_init.add_inputs(reversed(wembs))]\n",
    "    fw = [x.output() for x in fw_states]\n",
    "    bw = [x.output() for x in bw_states]\n",
    "    \n",
    "    # Output of the input gate\n",
    "    O = dy.parameter(pOutput)\n",
    "    \n",
    "    if mlp:\n",
    "        # Output of the hidden layer\n",
    "        H = dy.parameter(pHidden)\n",
    "    errs = []\n",
    "    pred_tags = []\n",
    "    \n",
    "    for f, b, t in zip(fw, reversed(bw), tags):\n",
    "        # The current input, and output from the previous state are combined\n",
    "        f_b = dy.concatenate([f,b])\n",
    "        if mlp:\n",
    "        # The previous LSTM cell outputs are squashed b/n -1 and 1\n",
    "            f_b = dy.tanh(H * f_b)\n",
    "        # squashed input is then multiplied element-wise by the output of the input gate. \n",
    "        r_t = O * f_b\n",
    "        if train:\n",
    "            err = dy.pickneglogsoftmax(r_t, t)\n",
    "            errs.append(err)\n",
    "        else:\n",
    "            out = dy.softmax(r_t)\n",
    "            chosen = np.argmax(out.npvalue())\n",
    "            pred_tags.append(vocab_tags.i2w[chosen])\n",
    "\n",
    "    O_template = dy.parameter(pOutputTemplate)\n",
    "    H_template = dy.parameter(pHiddenTemplate)\n",
    "    f_bt = dy.concatenate([fw_states[-1].s()[0], bw_states[-1].s()[0]])\n",
    "    f_bt = dy.tanh(H_template * f_bt)\n",
    "    r_tt = O_template * f_bt\n",
    "    pred_template = None\n",
    "    if train:\n",
    "        err = dy.pickneglogsoftmax(r_tt, template)\n",
    "        errs.append(err)\n",
    "    else:\n",
    "        out = dy.softmax(r_tt)\n",
    "        chosen = np.argmax(out.npvalue())\n",
    "        pred_template = vocab_templates.i2w[chosen]\n",
    "\n",
    "    return pred_tags, pred_template, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tagged_tokens(tokens, tags, template):\n",
    "    to_insert = {}\n",
    "    cur = (None, [])\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag != cur[0]:\n",
    "            if cur[0] is not None:\n",
    "                value = ' '.join(cur[1])\n",
    "                to_insert[cur[0]] = value\n",
    "            if tag == 'O':\n",
    "                cur = (None, [])\n",
    "            else:\n",
    "                cur = (tag, [token])\n",
    "        else:\n",
    "            cur[1].append(token)\n",
    "    if cur[0] is not None:\n",
    "        value = ' '.join(cur[1])\n",
    "        to_insert[cur[0]] = value\n",
    "\n",
    "    modified = []\n",
    "    for token in template.split():\n",
    "        modified.append(to_insert.get(token, token))\n",
    "\n",
    "    return ' '.join(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(data, builders, iteration, step):\n",
    "    if len(data) == 0:\n",
    "        print(\"No data for eval\")\n",
    "        return -1\n",
    "    correct_tags = 0.0\n",
    "    total_tags = 0.0\n",
    "    complete_match = 0.0\n",
    "    templates_match = 0.0\n",
    "    oracle = 0.0\n",
    "    for tokens, tags, template, complete in data:\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [0 for tag in tags]\n",
    "        pred_tags, pred_template, _ = build_tagging_graph(word_ids, tag_ids, 0, builders, False)\n",
    "        gold_tags = tags\n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            total_tags += 1\n",
    "            if gold == pred: correct_tags += 1\n",
    "        pred_complete = insert_tagged_tokens(tokens, pred_tags, pred_template)\n",
    "        if pred_complete == complete:\n",
    "            complete_match += 1\n",
    "        if pred_template == template:\n",
    "            templates_match += 1\n",
    "        if template in vocab_templates.w2i:\n",
    "            oracle += 1\n",
    "\n",
    "    tok_acc = correct_tags / total_tags\n",
    "    complete_acc = complete_match / len(data)\n",
    "    template_acc = templates_match / len(data)\n",
    "    oracle_acc = oracle / len(data)\n",
    "    \n",
    "    print(f\"Evaluation {iteration} - {step} => Tagging Accuracy: {tok_acc:.2%}% Template Accuracy: {template_acc:.2%}%\")\n",
    "    print(f\"Total Accuracy: {complete_acc:.2%}%\")\n",
    "    return complete_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [16:13<00:00, 20.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation End - test => Tagging Accuracy: 98.98%% Template Accuracy: 78.40%%\n",
      "Total Accuracy: 74.80%%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7479541734860884"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = 0\n",
    "loss = 0\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "# Number of examples to decode between logging\n",
    "log_freq = 1000000\n",
    "# Number of examples to decode between evaluation runs\n",
    "eval_freq = 1000000\n",
    "\n",
    "iters_since_best_updated = 0\n",
    "iters_without_improvement = 5\n",
    "steps = 0\n",
    "iters = 50\n",
    "\n",
    "#  Training in 50 epochs'\n",
    "for iteration in tqdm(range(iters)):\n",
    "    random.shuffle(train)\n",
    "    for tokens, tags, template, complete in train:\n",
    "        steps += 1\n",
    "\n",
    "        # Convert to indices\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [vocab_tags.w2i[tag] for tag in tags]\n",
    "        template_id = vocab_templates.w2i[template]\n",
    "\n",
    "        # Decode and update\n",
    "        _, _, errs = build_tagging_graph(word_ids, tag_ids, template_id, builders)\n",
    "        sum_errs = dy.esum(errs)\n",
    "        loss += sum_errs.scalar_value()\n",
    "        tagged += len(tag_ids)\n",
    "        sum_errs.backward()\n",
    "        trainer.update()\n",
    "\n",
    "        # Log status\n",
    "        if steps % log_freq == 0:\n",
    "            trainer.status()\n",
    "            print(f\"TrainLoss {iteration}-{steps}: {loss / tagged}\")\n",
    "            loss = 0\n",
    "            tagged = 0\n",
    "            sys.stdout.flush()\n",
    "        if steps % eval_freq == 0:\n",
    "            acc = run_eval(dev, builders, iteration, steps)\n",
    "            if best_dev_acc < acc:\n",
    "                best_dev_acc = acc\n",
    "                iters_since_best_updated = 0\n",
    "                print(\"New best Accuracy!\", acc)\n",
    "            sys.stdout.flush()\n",
    "    iters_since_best_updated += 1\n",
    "    if iters > 0 and iters_since_best_updated > 50:\n",
    "       print(f'Stopping at {steps} iterations as there have been {iters_without_improvement} iterations without improvement')\n",
    "       break\n",
    "        \n",
    "run_eval(test, builders, \"End\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most significant challenges were configuring the input data pipeline and ensuring that the words were being correctly tagged and encoded. It was also challenging to select appropriate hyperparameters that would ensure the model does not underperform or overfit. My first iteration of the model had an accuracy of ~40%.\n",
    "\n",
    "### Preprocessing\n",
    "The inputs to the model was a dictionary that mapped words to integers, with no meaningful relationships between the words. Word embeddings are dense, floating-point vectors that are a more powerful way to encode vocabulary. Word embeddings are used to map human language into a geometric space, so that geometric relationships between word vectors reflect the semantic relationships between words. This could have been used to improve model performance by enabling it to better understand context.\n",
    "\n",
    "### Building the model\n",
    "\n",
    "The performance of the model could also have been enhanced by using mechanisms such as Attention, beam search, and copying. Attention helps the network pay attention to specific words in the input sequence, such as table column names. Beam Search will enable the decoder to predict the most probable next word, by taking into account the probability each word at every step in the sequence. Copying allows the model to generate words by simply copying them from the input sequence. These mechanism will enable the model to perform well on query-based splits.\n",
    "\n",
    "### Tuning the model\n",
    "\n",
    "Dropout is a regularization method that prevents activation and weight updates for some input and recurrent connections to LSTM units during training. The dropout rate could've been adjusted to possibly improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@InProceedings{data-sql-advising,\n",
    "  author    = {Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev},\n",
    "  title     = {Improving Text-to-SQL Evaluation Methodology},\n",
    "  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n",
    "  month     = {July},\n",
    "  year      = {2018},\n",
    "  address   = {Melbourne, Victoria, Australia},\n",
    "  pages     = {351--360},\n",
    "  url       = {http://aclweb.org/anthology/P18-1033},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a modified version of the example [tagger code](https://github.com/clab/dynet/blob/master/examples/tagger/bilstmtagger.py) from the DyNet repository.\n",
    "\n",
    "It is available under an Apache 2 license."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
